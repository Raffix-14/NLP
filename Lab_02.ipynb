{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raffix-14/NLP/blob/main/Lab_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbYDu6cchK-l"
      },
      "source": [
        "# **Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Moreno La Quatra\n",
        "\n",
        "**Practice 2:** Word and Sentence Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F7DHpWknNZG"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MorenoLaQuatra/DeepNLP/blob/main/2022_2023/Practice_2_Word_and_Sentence_Embeddings.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbJsg8XEOudF"
      },
      "source": [
        "## Word Embedding \n",
        "\n",
        "![](https://qph.fs.quoracdn.net/main-qimg-3e812fd164a08f5e4f195000fecf988f)\n",
        "\n",
        "\n",
        "**Key takeaways** from lessons and in-class practices:\n",
        "- Word embeddings are able to map words into a semantic-aware vector space\n",
        "- There are multiple architectures for the generation of word embeddings\n",
        "- Each architecture has its advantages and disadvantages\n",
        "- Word embedding evaluation could be intrinsic (intermediate tasks) or extrinsic (downstream task)\n",
        "- It is possible to use pre-trained word embedding models or use large amount of text to train it from scratch\n",
        "- The use of pre-trained word embedding models is a common practice in NLP and removes the need of training a word embedding model from scratch (that could be very time consuming and computationally expensive)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-9kQCr4P8wP"
      },
      "source": [
        "### **Question 1**\n",
        "\n",
        "Train a new Word2Vec model using gensim with the text8 corpus available in the python package ([reference](https://radimrehurek.com/gensim/downloader.html)). Compute the training time for the model and store it for subsequent steps.\n",
        "\n",
        "**Hint:** you can use the following code to load the text8 corpus:\n",
        "\n",
        "```python\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "import time\n",
        "dataset = api.load(\"text8\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UW_G_mRAOt9e"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "od6-XXHkkkxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a4a5b2-cc67-4522-d94f-7c40c0a0eb7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "109.48493313789368\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "import time\n",
        "dataset = api.load(\"text8\")\n",
        "t0 = time.time()\n",
        "model = Word2Vec(dataset)\n",
        "print(time.time() - t0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mij5WNAARGMl"
      },
      "source": [
        "### **Question 2**:\n",
        "Perform **intrinsic** evaluation of the model for the task of word analogy by exploiting the data collection available [here](https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P2/google_analogies.csv). \n",
        "\n",
        "1. read CSV file\n",
        "2. group analogy entries by type (column: `type`)\n",
        "3. for each type entry (**in the lab, just set type=\"family\"** to reduce the required time) use the first 3 word vectors to compute the fourth\n",
        "    - Entry: `Athens,Greece,Baghdad,Iraq`\n",
        "    - `v(Greece) - v(Athens) + v(Baghdad) = res_v` \n",
        "    - Get the most similar vectors to `res_v`\n",
        "    - Compute in how many cases the correct word is among the top K (if `v[Iraq]` is among the K most similar words) with `K = 1, 3, 5, 10`\n",
        "\n",
        "$top(k) = \\dfrac{\\sum_{i=1}^{N} f(i)}{|E|}$\n",
        "\n",
        "where $f(i) = 1$ if the target word is among the top k and $f(i) = 0$ otherwise.\n",
        "\n",
        "$|E|$ is the total number of entries for the considered type.\n",
        "\n",
        "**Notes:**\n",
        "1. Try with the model trained on `text8`, is there any issue? If yes, how can you solve it?\n",
        "2. Test the model trained on Google News available in gensim.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VArA054TTX5y"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P2/google_analogies.csv\n",
        "!pip install --upgrade pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LDPl6w1ob3WX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffab4bd2-45d9-46dd-dc60-d591ee7e4733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "# Executing this cell could take ~5 minutes - you can write your code in the meantime\n",
        "import gensim.downloader\n",
        "w2v_google_news_model = gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "6ykQt1UPkjfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a45f8054-f7cc-4112-c44b-5f1eb1478779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Baghdad', 0.7489827275276184)]\n",
            "[('Baghdad', 0.7489827275276184), ('Iraqi', 0.6727433204650879), ('Mosul', 0.6652752757072449)]\n",
            "[('Baghdad', 0.7489827275276184), ('Iraqi', 0.6727433204650879), ('Mosul', 0.6652752757072449), ('Iraq', 0.6355191469192505), ('Iraqis', 0.6133478879928589)]\n",
            "[('Baghdad', 0.7489827275276184), ('Iraqi', 0.6727433204650879), ('Mosul', 0.6652752757072449), ('Iraq', 0.6355191469192505), ('Iraqis', 0.6133478879928589), ('Samarra', 0.6069650053977966), ('Sunni_Arab', 0.6064962148666382), ('Basra', 0.5986490845680237), ('Fallujah', 0.5971603989601135), ('Anbar', 0.5966798067092896)]\n",
            "2 \n",
            " Accuracy: 0.5\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"google_analogies.csv\")\n",
        "df_family = df[df['type']=='family']\n",
        "v = w2v_google_news_model\n",
        "K = [1, 3, 5, 10]\n",
        "\n",
        "res_v = v['Greece'] - v['Athens'] + v['Baghdad']\n",
        "count = 0\n",
        "for k in K:\n",
        "  top_similar = v.most_similar(res_v, topn=k)\n",
        "  print(top_similar)\n",
        "  if 'Iraq' in list(zip(*top_similar))[0]:\n",
        "    count += 1\n",
        "print(count, '\\n', 'Accuracy:', count/len(K))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noRDD9hWmyZO"
      },
      "source": [
        "### **Question 3:**\n",
        "\n",
        "Train a new FastText model using gensim with text8 corpus available in the python package ([reference](https://radimrehurek.com/gensim/downloader.html)). Compute the training time for the model and store it for subsequent steps. \n",
        "\n",
        "- Is there any significant difference in training time if compared with Word2Vec training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "UPmwnDgWoLcR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eee1cad1-f9d3-44fa-c131-aea526770be5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "436.4086244106293\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import FastText\n",
        "import time\n",
        "dataset = api.load(\"text8\")\n",
        "t0 = time.time()\n",
        "model = FastText(dataset)\n",
        "print(time.time() - t0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoAQbgL1oiEM"
      },
      "source": [
        "### **Question 4:**\n",
        "Provide the same evaluation done in Question 2 for the FastText model. In this case, you can use the same type of analogy (family) and the same K values.\n",
        "\n",
        "**Notes:**\n",
        "- Try with the model trained on `text8`, is there any issue? What does it mean?\n",
        "- Test the model trained on Wikipedia+News available in gensim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "gAR9t6Y9nNZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb8251e-1ff6-49e5-e62f-857d968bd659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ],
      "source": [
        "# Executing this cell could take ~5 minutes - you can write your code in the meantime\n",
        "import gensim.downloader\n",
        "ft_wiki_news_model = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "0JvL6Tizkn6j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a2144d-a308-4de7-9ecc-f3f3f18eb907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Baghdad', 0.7944707274436951)]\n",
            "[('Baghdad', 0.7944707274436951), ('Iraq', 0.7864524126052856), ('Mosul', 0.6864204406738281)]\n",
            "[('Baghdad', 0.7944707274436951), ('Iraq', 0.7864524126052856), ('Mosul', 0.6864204406738281), ('Kuwait', 0.6606603264808655), ('Mesopotamia', 0.6495900750160217)]\n",
            "[('Baghdad', 0.7944707274436951), ('Iraq', 0.7864524126052856), ('Mosul', 0.6864204406738281), ('Kuwait', 0.6606603264808655), ('Mesopotamia', 0.6495900750160217), ('Iraq-Syria', 0.6473897695541382), ('Syria', 0.645632266998291), ('Baghdady', 0.6396152377128601), ('Al-Basrah', 0.6377001404762268), ('Iraq-Kuwait', 0.6375606060028076)]\n",
            "3 \n",
            " Accuracy: 0.75\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"google_analogies.csv\")\n",
        "df_family = df[df['type']=='family']\n",
        "v = ft_wiki_news_model\n",
        "K = [1, 3, 5, 10]\n",
        "\n",
        "res_v = v['Greece'] - v['Athens'] + v['Baghdad']\n",
        "count = 0\n",
        "for k in K:\n",
        "  top_similar = v.most_similar(res_v, topn=k)\n",
        "  print(top_similar)\n",
        "  if 'Iraq' in list(zip(*top_similar))[0]:\n",
        "    count += 1\n",
        "print(count, '\\n', 'Accuracy:', count/len(K))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRneIhkoxV_O"
      },
      "source": [
        "### **Question 5** (optional) \n",
        "\n",
        "Provide a complete evaluation of the best performing models (Word2Vec and FastText) by leveraging the complete dataset of analogy entries. In this case, you should use all the analogy types and all you can use the same K values provided in Question 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "64CHB3Umkq_1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "2d0a2d0b-f5ed-4914-a85b-776e5184817c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-33b152dc2f40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'word2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'word3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'right_format_analogies.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2v_google_news_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_word_analogies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalogies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"right_format_analogies.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_wiki_news_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_word_analogies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalogies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"right_format_analogies.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mevaluate_word_analogies\u001b[0;34m(self, analogies, restrict_vocab, case_insensitive, dummy4unknown, similarity_function)\u001b[0m\n\u001b[1;32m   1348\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Missing section header before line #%i in %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalogies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcase_insensitive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Missing section header before line #0 in right_format_analogies.txt"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"google_analogies.csv\")\n",
        "df[['word1','word2','word3','target']].to_csv('right_format_analogies.csv', index=False)\n",
        "\n",
        "a = w2v_google_news_model.evaluate_word_analogies(analogies=\"right_format_analogies.txt\")\n",
        "b = ft_wiki_news_model.evaluate_word_analogies(analogies=\"right_format_analogies.txt\")\n",
        "print(a, '\\n', b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-AuxU7YhU7D"
      },
      "source": [
        "## Sentence Embeddings\n",
        "\n",
        "Sentence embeddings are a way to represent a sentence in a vector space. The vector space is usually learned from a large corpus of text. They are used in many NLP tasks, such as text classification, text similarity, and question answering. In this practice, we will use and interact both with Doc2Vec and InferSent models.\n",
        "\n",
        "Key takeaways from lessons and in-class practices:\n",
        "- Doc2Vec is an extension of the Word2Vec framework.\n",
        "- It incorporates Document ID to obtain a more accurate representation of a document/paragraph.\n",
        "- Training document vectors are pre-computed, however you can infer vectors for new documents.\n",
        "- InferSent exploit a deep learning architecture to supervisedly learn sentence representations.\n",
        "- InferSent vectors could exploit both Word2Vec or FastText as word embedding models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60_q4G-xyooQ"
      },
      "source": [
        "### **Question 6**\n",
        "\n",
        "Train a novel Doc2Vec model using the [APIs provided by gensim](https://radimrehurek.com/gensim/models/doc2vec.html) with text8 corpus. \n",
        "\n",
        "- Which is the training time for the model? Is it comparable with Word2Vec and FastText training time?\n",
        "\n",
        "NB. **Store** the model to a file for subsequent steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eIj2s1skr-s"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCqY7VFOwsK9"
      },
      "source": [
        "### **Question 7 (Doc2Vec qualitative evaluation)**\n",
        "Perform some **qualitative** experiments by computing the cosine similarities between sentences composed by yourself.\n",
        "For example, you can use the following sentences:\n",
        "\n",
        "```python\n",
        "s1 = \"The president of the United States is Donald Trump\"\n",
        "s2 = \"The president of the United States is Joe Biden\"\n",
        "s3 = \"United States is a country\"\n",
        "s4 = \"The cell phone is a device\"\n",
        "```\n",
        "\n",
        "Please try to interact with the model by providing different sentences and check the results. Is the model able to capture the semantic meaning of the sentences? Are you satisfied with the results?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrYt1-LckvSI"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnor1_VWnNZV"
      },
      "source": [
        "### **Question 8** \n",
        "\n",
        "Load the InferSent model provided by Facebook Research ([reference](https://github.com/facebookresearch/InferSent)) and perform the same qualitative evaluation done in Question 7. In this case, you can use the InferSent pretrained model (v2) - [reference](https://github.com/facebookresearch/InferSent).\n",
        "\n",
        "Try to find some sentences for which InferSent is able to capture the semantic meaning of the sentences as opposed to Doc2Vec. Are you satisfied with the results? Which model is able to better capture the semantic meaning of the sentences? What can be the reason for this?\n",
        "\n",
        "**Notes:**\n",
        "Please find below the code to download the InferSent model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOPAa0rdnNZW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# InferSent download required files\n",
        "\n",
        "! mkdir fastText\n",
        "! curl -Lo fastText/crawl-300d-2M.vec.zip https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
        "! unzip fastText/crawl-300d-2M.vec.zip -d fastText/\n",
        "! mkdir encoder\n",
        "! curl -Lo encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\n",
        "! git clone https://github.com/facebookresearch/InferSent.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNHLHfGinNZX"
      },
      "outputs": [],
      "source": [
        "from InferSent.models import InferSent\n",
        "import torch\n",
        "V = 2\n",
        "MODEL_PATH = 'encoder/infersent%s.pkl' % V\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
        "infersent = InferSent(params_model)\n",
        "infersent.load_state_dict(torch.load(MODEL_PATH))\n",
        "\n",
        "W2V_PATH = 'fastText/crawl-300d-2M.vec'\n",
        "infersent.set_w2v_path(W2V_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-gdPpfMnNZY"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBqLp1LnTeME"
      },
      "source": [
        "### **Question 9** (Extrinsic Evaluation)\n",
        "\n",
        "**Extrinsic** evaluation aims at measuring the performance of the word/sentence/paragraph embedding model when used in a downstream task. In this case, we will use the model to perform a text classification task.\n",
        "We can use different configuration, training corpora or even different models to build a complete architecture for the task at hand.\n",
        "\n",
        "For this practice we use the text classification dataset available [here](https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P2/news_headline_classification.csv) - [source: Kaggle](https://www.kaggle.com/rmisra/news-category-dataset). It contains news headlines and the corresponding category. The dataset is composed by 200846 divided into multiple categories (e.g. politics, business, sports, etc.).\n",
        "\n",
        "**Note:** consider using just the first 10.000 headlines to reduce runtime during the lab. You can use the complete data collection at home to achieve better results.\n",
        "\n",
        "Compute the accuracy of 3 classification models each one built with one of the models introduced in this practice:\n",
        "- Word2Vec model pretrained on Google News corpus\n",
        "- FastText model pretrained on Wikipedia+News corpus\n",
        "- **[Optional]** Doc2Vec model pretrained on Text8 corpus\n",
        "- **[Optional]** InferSent pretrained model (v2) - [reference](https://github.com/facebookresearch/InferSent)\n",
        "\n",
        "The procedure to create a classification system is sketched below:\n",
        "1. Choose a machine learning (multi-class) classifier (e.g., MLP)\n",
        "2. Split the data collection in train/test (80%/20%)\n",
        "3. Use text vectors obtained by pretrained model as input of the classifier\n",
        "4. Measure the accuracy of the classification system\n",
        "5. Repeat step 3-4 using different embedding models \n",
        "\n",
        "\n",
        "**Note:** For word embedding models you must use an aggregation strategy to obtain a single vector for each sentence. You can use the average of the word vectors or the sum of the word vectors. In both cases, the output vector can be used as input of the classifier.\n",
        "\n",
        "Report the performance of each classification pipeline. Which model has better performance? Why? Try to elaborate on the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qp2k7npVeoTB"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P2/news_headline_classification.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKQZIX6ckzcA"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VBbEBO6xmEL"
      },
      "source": [
        "**Word2Vec + Average aggregation function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmjcUbRCk2Ll"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTLSmUJXxeId"
      },
      "source": [
        "**FastText + Average aggregation function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtMLnlTyk4J7"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBjSrAx9xrON"
      },
      "source": [
        "**Doc2Vec (Text8)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbcc4qWmk9Jt"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7DkgKMcxrIy"
      },
      "source": [
        "**InferSent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfKMM5E9lDeY"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}